---
title:  "Boostcamp AI tech NLP 과정 3주차 노트"
categories: boostcamp-aitech
tag: [Boostcamp-aitech, NLP, Python]
author_profile: false
sidebar:
    nav: "docs"
search: true
use_math: true
---

## MLP

### MLP 의 Hidden layer 들은 어떤 목적으로 사용되나요?

- 복잡한 문제 해결능력 향상 (이거는 Activation function 이 있어야만 하는 것 아닌가?  은닉층은 이미 달려있는걸 말하는 것이다.) 
- 네트워크 깊이 증가


## Optimization

### Momentum

Local minimum 에서 벗어나 Global minimum 을 찾기위해 사용할수있는 Optimization 기술


## CNN

### 파라미터 갯수

CNN 모델에 3*3*64의 filter가 32개 있을 때, 모든 filter의 파라미터의 수는 (3*3*64=576)이며, bias를 고려하면 (576+1=577)이다.. 32개의 필터가 있으므로, (577*32=18464)이다.

### CNN에서 여러 채널을 사용하는 주된 이유

여러 채널을 사용하면 다양한 공간적 특징을 동시에 학습하여 모델의 성능을 향상시킬 수 있다.

### Semantic Segmentation

1. 픽셀 단위로 레이블을 할당한다.

2. 결과를 사용하여 도로, 건물, 사람 등의 영역을 식별할 수 있다.

3. 이미지 내의 영역을 이해하는 데 도움을 준다.

4. 이미지의 개별 객체를 구별한다.

### Fully Convolutional Network (FCN)

1. pooling 레이어와 convolutional 레이어를 모두 사용한다.

2. semantic segmentation 작업을 위해 설계되었다. (Heat map)

3. 이미지의 임의의 크기를 처리할 수 있다.

4. 전통적인 Convolutional Neural Networks(CNN)의 완전 연결 레이어를 convolutional 레이어로 바꾼다.

5. FCN은 고정된 크기의 출력 대신 픽셀 수준의 분류를 제공한다.

### Deconvolution

1. 컴퓨터 비전 및 딥 러닝 컨텍스트에서의 'deconvolution'은 주로 업샘플링 및 해상도 복구에 사용되는 연산을 의미한다.

2. 실제 역합성곱 연산을 나타내지는 않는다. (그냥 이해하기 쉽게 하려고)

3. 입력의 각 픽셀을 확대하여 출력을 생성한다.


## RNN

### Sequential Model

- Sequential Model은 레이어를 선형적으로 쌓는 구조를 가지므로, 복잡한 다중 입력, 다중 출력 모델을 직접적으로 구성하기 어렵다.

- 다중 입력이나 다중 출력이 필요한 경우 함수형 API를 사용하는 것이 더 적합하다.

- Sequential Model은 레이어를 순차적으로 추가하여 모델을 구축하는 간단하고 선형적인 구조를 가지며, 이로 인해 단일 입력 및 단일 출력의 순차적인 모델을 쉽게 만들 수 있다.

### LSTM

- LSTM은 RNN의 한 종류로서, 게이트 메커니즘이 포함된다.

- forget gate, input gate, output gate로 이루어진 세 가지 주요 게이트를 가지고 있다.

- LSTM의 메모리 셀은 정보를 저장, 삭제, 읽을 수 있는 능력을 가지고 있어 장기적인 의존성 문제를 효과적으로 다룰 수 있다.

- LSTM은 장기적인 의존성을 처리하는 데 뛰어나지만, 특정 작업이나 데이터셋에 따라서는 기본 RNN이나 다른 모델 구조가 더 나은 성능을 낼 수도 있다.


## Generative Models

### Autoregressive Model

- Autoregressive Model은 여러 변수 간의 시간적인 의존성을 모델링하는데 효과적이다. 연속 변수로의 확장성이 좋으며, Gaussian Model과 같은 확률 모델을 사용하여 예측을 수행할 수 있다.

- Autoregressive Model은 주어진 과거 값들에 조건을 둔 현재 값의 확률을 모델링하는 것이다. 다시 말해, 모델은 과거의 데이터 포인트들을 바탕으로 현재 값의 조건부 확률을 계산하려고 시도한다.

- NADE는 데이터의 결합 확률 분포를 모델링하는 데 사용되는 신경망 기반의 방법론이다. 데이터의 각 차원에 대한 조건부 확률을 학습하여 전체 데이터의 확률 분포를 추정한다. 이러한 접근법은 복잡한 밀도 추정 문제에 효과적으로 사용될 수 있다.

### Variational Auto Encoder (VAE)

Variational Autoencoder (VAE)는 잠재 변수 모델로, 데이터를 압축된 표현(잠재 공간: latent vector)으로 변환한 후 다시 원본과 유사한 출력을 생성한다. 이 잠재 공간에서의 샘플링을 통해 새로운 데이터나 예측이 생성될 수 있게 되는 것이 VAE의 주요 특징 중 하나이다.


### Gradient Vanishing

Gradient 소실 문제로 인해 네트워크의 앞부분 레이어들이 제대로 학습되지 않을 수 있다.

### Number of params in CNN

Convolutional Layer가 있으며, Filter 크기가 3*3, 입력 채널이 16, filter 수가 32일 때, 이 Layer의 파라미터 수는 (Filter 가로 크기 $\times$ Filter 세로 크기 $\times$ 입력 채널 수 $+$ 1(bias)) $\times$ 필터 수 $=$ $(3 \times 3 \times 16 + 1) \times 32 = 4640$

### Regularization

Bagging은 모델의 과대적합을 방지하고, 일반화 성능을 향상시키기 위해 사용된다.

### Detection Models

R-CNN은 Selective search 알고리즘을 사용하여 Region proposal (bounding box) 를 얻고, 각 후보 영역마다 CNN을 forward 하여 특징을 추출한다. 이와 달리, Fast R-CNN은 전체 이미지를 한 번의 forward path 로 단방에 처리하고 그 결과인 feature map 으로부터 Region of Interest (RoI) pooling을 사용하여 특징을 추출한다. 이러한 접근 방식으로 인해 Fast R-CNN은 R-CNN에 비해 훨씬 빠르다.

### Evidence Lower Bound (ELBO)의 역할 및 특성 with Variational Auto Encoder (VAE)

- ELBO는 실제 데이터의 로그 가능도와 잠재 변수의 사후 분포와의 Kullback-Leibler 발산 사이의 차이를 나타낸다.

- ELBO를 최대화하는 것은 원래의 문제인 로그 가능도를 최대화하는 것과 동일하다.

- ELBO는 원래의 데이터 생성 확률의 하한으로 간주되며, 이를 최대화함으로써 원래의 목표를 간접적으로 최적화한다.

- VAE에서 ELBO의 최대화는 실제 사후 분포와 근사 사후 분포 간의 발산을 최소화하는 것을 의미한다.

- VAE에서는 근사 사후 분포를 사용하여 실제 사후 분포를 근사하려고 한다. ELBO의 최대화는 이 근사를 개선하는 데 중요한 역할을 하지만, 이 과정에서 모든 잠재 변수의 사후 분포를 동일한 가우스 분포로 고정시키지는 않는다.

### GAN

- Discriminator는 가짜와 진짜 샘플을 구별하는 모델로, 진짜 샘플에 대한 확률을 최대화하려고 한다.

- Generator는 실제 데이터 분포와 유사한 샘플을 생성하는 모델로, Discriminator가 가짜 샘플을 진짜로 잘못 분류하게 만드는 것을 목표로 한다.

- GAN의 전체 목적함수는 Discriminator의 로그 가능도를 최소화하고, Generator의 로그 가능도를 최대화하는 것을 포함한다.

- GAN의 목적함수는 Generator를 최적화하기 위한 것이므로, Discriminator의 성능에 영향을 받지 않는다.

- GAN의 학습은 두 네트워크, Generator와 Discriminator 사이의 경쟁을 기반으로 한다. Discriminator의 성능이 중요하며, 올바르게 학습되어야 Generator도 효과적으로 학습될 수 있다. 만약 Discriminator가 너무 강력하거나 약하면 학습은 불안정해질 수 있다.