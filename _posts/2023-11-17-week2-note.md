---
title:  "Boostcamp AI tech NLP 과정 2주차 노트"
categories: boostcamp-aitech
tag: [Boostcamp-aitech, NLP, Python]
author_profile: false
sidebar:
    nav: "docs"
search: true
use_math: true
---

## Pytorch

### Pytorch 모델 학습의 기본 구조

- `output = model(data)`: nn.Module의 forward 메서드를 통해 입력값에 대한 결괏값을 연산한다.

- `loss = loss_function(output, label)`: 결과 값과 정답 간의 loss 값을 계산한다.

- `optimizer.zero_grad()`: 이전에 계산된 gradient를 0으로 초기화한다.

- `loss.backward()`: loss 값에 대한 weight와 bias의 gradient를 계산한다.

- `optimizer.step()`: `loss.backward()`를 통해 계산된 gradient를 사용하여 파라미터를 업데이트한다.



### Pytorch 프로젝트의 흐름

1. 초기 PyTorch 환경 설정 및 필요한 라이브러리 설치

2. DataLoader를 사용해 데이터 배치를 준비

3. 모델 아키텍처와 loss 함수, 옵티마이저를 정의

4. DDP(Distributed Data Parallelization)를 이용하여 여러 GPU를 사용해 모델 학습

5. Wandb를 통해 학습 중인 모델의 성능을 모니터링

6. GPU 메모리 문제 발생 시, 관련 에러를 해결

7. 다양한 하이퍼파라미터를 시도하며 모델 성능 튜닝
   - 요즘은 데이터양 자체가 많아진 관계로 하이퍼 파라미터 튜닝단계까지는 잘 오지 않음
   - 모델의 성능이 어느정도 나오는 상태에서 조금더 잘나오게 하기위한 방법




### 모델 성능 최적화와 학습관리 방법

1. 모델의 가중치를 정기적으로 저장하고, 가장 좋은 성능을 보인 시점의 가중치를 복원하여 사용한다.

2. TensorBoard와 같은 도구를 사용하여 학습 중인 모델의 성능, 손실 등의 지표를 시각화한다.

3. 모든 에포크마다 가중치의 변화를 출력하고, 이를 통해 모델이 수렴하는지 확인한다. (보통 1 epoch 돌때마다 한번씩)



### 파라미터 크기

총 파라미터의 수는 601,809개며, tensor의 데이터 타입은 `torch.float` (`torch.float32`) 이므로 32-비트 부동 소수점을 가지는 tensor이다. 32-비트는 4 바이트이므로 2,407,236 바이트의 용량을 차지한다.

문제의 선택지에서는 MB 단위로 통일했기 때문에, $1024^2$ 을 나눠 단위를 맞추면 답은 2.30 MB이다.

만약 1,024로 나누는 것이 아닌 1,000으로 나눈다면 단위는 MiB로 표시해야한다.

### Backward

- PyTorch에서 backward 메서드는 계산 그래프를 통해 gradient를 역전파한다. 이 과정에서, `requires_grad=True`로 설정된 tensor의 `.grad` 속성에 gradient가 누적된다. 즉, **`.grad` 속성에 이미 값이 있다면, 새로 계산된 gradient가 기존 값에 더해진다.**

- backward는 loss에 대해 사용된 모든 가중치를 미분한다. 학습 가능한 파라미터 tensor의 크기가 torch.Size([2,3])이라면, 총 $2 \times 3$ 크기 즉, 6개의 gradient 값이 모두 존재해야한다. .grad 차원의 크기는 학습 가능한 파라미터 tensor의 크기와 동일하다.



### Gradient Accumulation

```python
import torch
from torch import nn
from torch import optim

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(in_features=10, out_features=10)

    def forward(self, x):
        return self.fc(x)

model = SimpleModel()
optimizer = optim.Adam(model.parameters(), lr=.001)

accumulation_steps = 4
for i, data in enumerate(dataloader):
    inputs, labels = data
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()

    if (i+1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

Gradient Accumulation은 여러 미니배치에 대한 그래디언트를 축적하여, 그래디언트 업데이트를 주기적으로 수행하는 기술이다. (매 mini-batch 마다 gradient 를 계산하는것이 아니라)

보통은 이전스텝의 가중치를 0으로 만들고 (`zero_grad()`: 그래디언트 초기화하는 함수), 모델의 가중치를 update (`step()`: 계산된 그래디언트로 가중치를 update 하는 함수) 을 진행했지만, 이 경우에는 몇 step 의 그래디언트가 쌓여있으므로 우선 update 를 하고, 다음 누적을 위해 초기화 시켜준다.



### 모드

```python
import torch
from torch import nn

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(in_features=10, out_features=10)

    def forward(self, x):
        return self.fc(x)

model_path = "saved_model.ph" # or .pth
model = SimpleModel()
model.load_state_dict(torch.load(model_path))

# 추론 모드
model.eval()

input_tensor = torch.rand((1, 10))
with torch.inference_mode():
    output = model(input_tensor)

print(output)
```

1. 모델 가중치 불러오기: `torch.load` 함수는 저장된 모델의 state dictionary를 불러오는 데 사용된다. 이 state dictionary는 모델의 각 계층에 대한 매개 변수 정보를 담고 있다.

2. 추론 모드 설정: 모델 학습 시에는 일부 계층들(예: dropout, batch normalization 등)이 다르게 동작한다. 그러나 모델을 평가하거나 추론할 때는 이러한 계층들이 일관된 방식으로 동작해야 한다. 이를 위해 model.eval() 메서드를 사용하여 모델을 추론 모드로 설정한다. 이로 인해, dropout 계층은 드롭아웃을 적용하지 않게 되며, batch normalization 계층은 학습 시에 계산된 실행 중인 평균 및 분산을 사용하게 된다.

3. 그래디언트 계산 비활성화: `torch.no_grad()` $\rightarrow$ 현재 `torch.inference_mode()` 는 autograd 시스템이 추론 중에 발생하는 연산에 대한 그래디언트 정보를 추적하지 않도록 하는 컨텍스트이다. 추론 단계에서는 가중치 업데이트가 필요하지 않으므로 그래디언트 정보를 저장할 필요가 없다. 이로 인해 메모리 사용량이 줄고, 추론 속도도 향상된다.



### GPU 갯수 호환성 문제

여러 GPU에서 학습된 모델을 단일 GPU 환경에서 불러오려 할 때, 주로 발생하는 문제는 device 문제다. PyTorch에서 제공하는 `torch.load` 함수에 `map_location` 인자를 사용하여 모델을 불러올 device를 지정함으로써 이 문제를 해결할 수 있다. 이를 통해, 여러 GPU 환경에서 학습된 모델도 단일 GPU나 CPU 환경에서 문제없이 불러와 사용할 수 있게 된다.